{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_origin, Y_train_origin), (X_test_origin, Y_test_origin) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_origin.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_origin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_origin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_origin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a picture\n",
    "index = 50\n",
    "plt.imshow(X_train_origin[index])\n",
    "print (\"y = \" + str(Y_train_origin[index]))\n",
    "print (\"x = \" + str(X_train_origin[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## store all the shapes..\n",
    "m_train_x_size = X_train_origin.shape[0]\n",
    "m_train_y_size = Y_train_origin.shape[0]\n",
    "m_test_x_size = X_test_origin.shape[0]\n",
    "m_test_y_size = Y_test_origin.shape[0]\n",
    "\n",
    "print (\"Number of training examples in x: m_train_x = \" + str(m_train_x_size))\n",
    "print (\"Number of training examples in y: m_train_y = \" + str(m_train_y_size))\n",
    "print (\"Number of test examples in x: m_test_x = \" + str(m_test_x_size))\n",
    "print (\"Number of test examples in x: m_test_y = \" + str(m_test_y_size))\n",
    "print (\"training input image shape in pixel = \" + str(X_train_origin.shape[1:]))\n",
    "print (\"test input image shape in pixel = \" + str(X_test_origin.shape[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## flaten the array and \n",
    "## reshape from (no of images, width, height, color channel) to (width * height * color channel, no of images)\n",
    "\n",
    "X_train_flatten = X_train_origin.reshape(X_train_origin.shape[0], -1).T\n",
    "X_test_flatten = X_test_origin.reshape(X_test_origin.shape[0], -1).T\n",
    "\n",
    "Y_train_flatten = Y_train_origin.reshape(Y_train_origin.shape[0], 1).T\n",
    "Y_test_flatten = Y_test_origin.reshape(Y_test_origin.shape[0], 1).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep all training pixel values between 0 and 1\n",
    "X_train = X_train_flatten/255\n",
    "X_test = X_test_flatten/255\n",
    "\n",
    "Y_train = Y_train_flatten\n",
    "Y_test = Y_test_flatten\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters..\n",
    "def initialize_parameters(size_of_input_layer, size_of_hidden_layer_1, size_of_output_layer):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(size_of_hidden_layer_1, size_of_input_layer) * 0.01\n",
    "    \n",
    "    b1 = np.zeros((size_of_hidden_layer_1, 1), dtype= float)\n",
    "    \n",
    "    W2 = np.random.randn(size_of_output_layer, size_of_hidden_layer_1) * 0.01\n",
    "    \n",
    "    b2 = np.zeros((size_of_output_layer, 1), dtype= float)\n",
    "    \n",
    "\n",
    "    assert(W1.shape == (size_of_hidden_layer_1, size_of_input_layer))\n",
    "    \n",
    "    assert(b1.shape == (size_of_hidden_layer_1, 1))\n",
    "    \n",
    "    assert(W2.shape == (size_of_output_layer, size_of_hidden_layer_1))\n",
    "           \n",
    "    assert(b2.shape == (size_of_output_layer, 1))\n",
    "\n",
    "    parameters = {\n",
    "        \"W1\":W1,\n",
    "        \"b1\":b1,\n",
    "        \"W2\":W2,\n",
    "        \"b2\":b2\n",
    "    }\n",
    "           \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward prop section start..\n",
    "\n",
    "$A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. \n",
    "\n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â \\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "def Sigmoid(Z):\n",
    "    res = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    cache = Z\n",
    "    return res, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax\n",
    "def Softmax(Z):\n",
    "    res = np.exp(Z - Z.max())\n",
    "    res = res / np.sum(res, axis=0)\n",
    "    \n",
    "    cache = Z\n",
    "    return res, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Activation Functions and results..\n",
    "def Forward_Activation(A_prev, W, b, activation_function_to_apply):\n",
    "    \n",
    "    Z, linear_cache = Linear_Forward_Pass_Calculate_Z(A_prev, W, b)\n",
    "    \n",
    "    if activation_function_to_apply == 'sigmoid':\n",
    "        A, activation_cache = Sigmoid(Z)          # activation_cache contains Z. It will be used in back propagation..\n",
    "    elif activation_function_to_apply == 'softmax':\n",
    "        A, activation_cache = Softmax(Z)\n",
    "    \n",
    "    assert(A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass to get Z where Z = WT*A + b ....\n",
    "def Linear_Forward_Pass_Calculate_Z(A_prev, W, b): \n",
    "        # A = activation of previous layer\n",
    "        # W = Weights of current layer\n",
    "        # b = bias of current layer\n",
    "\n",
    "    Z = np.dot(W,A_prev) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    \n",
    "    cache = (A_prev, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function\n",
    "def Compute_cost(Y_hat, Y): #Y_hat = prediction, Y = actual Output..\n",
    "    \n",
    "    m = Y.shape[1] # m = number_of_example_data\n",
    "    \n",
    "    cost = (-1/m)*np.sum(Y*np.log(Y_hat) + (1-Y)*np.log(1-Y_hat))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "                                 # np.squeeze()  = Remove axes of length one from a.\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backprop section start..\n",
    "\n",
    " compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n",
    " $$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward sigmoid..\n",
    "def Backward_Sigmoid(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward softmax..\n",
    "def Backward_Softmax(dA_Final, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    res = np.exp(Z - Z.max())\n",
    "    res = res / np.sum(res, axis=0) * (1 - res / np.sum(res, axis=0))\n",
    "    \n",
    "    dZ = dA_Final * res\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward Activation.. \n",
    "def Backward_activation(dA, linear_and_activation_cache, backwad_activation_to_apply):\n",
    "    \n",
    "    linear_cache, activation_cache = linear_and_activation_cache\n",
    "    \n",
    "    if backwad_activation_to_apply == \"sigmoid\":\n",
    "        \n",
    "        dZ = Backward_Sigmoid(dA, activation_cache)\n",
    "        \n",
    "        dA_prev, dW, db = Linear_Backward_Pass_Calculate_dW_A_db(dZ, linear_cache)\n",
    "        \n",
    "    elif backwad_activation_to_apply == \"softmax\":\n",
    "        \n",
    "        dZ = Backward_Softmax(dA, activation_cache)\n",
    "        dA_prev, dW, db = Linear_Backward_Pass_Calculate_dW_A_db(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear backward pass..\n",
    "def Linear_Backward_Pass_Calculate_dW_A_db(dZ, linear_cache):\n",
    "    \n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1/m)*np.dot(dZ,(A_prev.T))\n",
    "    db = (1/m)*np.sum(dZ, axis = 1, keepdims=True)\n",
    "    dA_prev = np.dot((W.T),dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update parameters\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate): #parameters are, W1, b1, W2, b2 ..[weights and biases]\n",
    "    \n",
    "    L = len(parameters) // 2   # number of layers in the neural network\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*gradients[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*gradients[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model\n",
    "\n",
    "Overall steps:\n",
    "    1. Initialize parameters / Define hyperparameters\n",
    "    2. Loop for num_iterations:\n",
    "        a. Forward propagation\n",
    "        b. Compute cost function\n",
    "        c. Backward propagation\n",
    "        d. Update parameters (using parameters, and grads from backprop) \n",
    "    4. Use trained parameters to predict labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Two_Layer_Neural_Model(X, Y, layers_dimensions, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    gradients = {}\n",
    "    costs = []\n",
    "    \n",
    "    m = X.shape[1]  # no of examples\n",
    "    \n",
    "    (size_of_input_layer, size_of_hidden_layer1, size_of_output_layer) = layers_dimensions \n",
    "    \n",
    "    #initialize\n",
    "    parameters = initialize_parameters(size_of_input_layer, size_of_hidden_layer1, size_of_output_layer)\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        #forward prop\n",
    "        A1, cache1 = Forward_Activation(X, W1, b1, activation_function_to_apply = \"sigmoid\")\n",
    "        A2, cache2 = Forward_Activation(A1, W2, b2, activation_function_to_apply = \"softmax\")\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = Compute_cost(A2, Y)\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = 2 * (A2 - Y) / A2.shape[0]\n",
    "        \n",
    "        #calculate gradients..\n",
    "        dA1, dW2, db2 = Backward_activation(dA2, cache2, backwad_activation_to_apply = \"softmax\")\n",
    "        dA0, dW1, db1 = Backward_activation(dA1, cache1, backwad_activation_to_apply = \"sigmoid\")\n",
    "        \n",
    "        #store gradients..\n",
    "        gradients['dW1'] = dW1\n",
    "        gradients['db1'] = db1\n",
    "        gradients['dW2'] = dW2\n",
    "        gradients['db2'] = db2\n",
    "        \n",
    "        #pdate parameters..\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 10 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 10 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "n_x = 784     # num_px * num_px\n",
    "n_h = 128\n",
    "n_y = 10\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = Two_Layer_Neural_Model(X_train, Y_train, layers_dimensions = (n_x, n_h, n_y), num_iterations = 100, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    \n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
